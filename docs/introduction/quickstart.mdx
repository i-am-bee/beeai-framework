---
title: "Quickstart"
description: "Get up and running with the BeeAI framework"
icon: "rocket"
---

<Steps>
   <Step title="Clone a starter repo">

  Get started quickly with the a BeeAI Framework starter template.
  for [Python](https://github.com/i-am-bee/beeai-framework-py-starter) or [Typescript](https://github.com/i-am-bee/beeai-framework-ts-starter).

  <CodeGroup>
  ```bash Python
  git clone https://github.com/i-am-bee/beeai-framework-py-starter.git
  cd beeai-framework-py-starter
  ```

  ```bash TypeScript
  git clone https://github.com/i-am-bee/beeai-framework-ts-starter.git
  cd beeai-framework-ts-starter
  nvm install && nvm use
  npm ci
  ```

  </CodeGroup>

  </Step>

   <Step title="Install BeeAI framework">

  <CodeGroup>

  ```bash Python
  pip install beeai-framework
  ```

  ```bash TypeScript
  npm install beeai-framework
  ```

  </CodeGroup>

   </Step>
   <Step title="Create your project file">

   Copy the following code into a file named quickstart.py for Python or quickstart.ts for TypeScript.

  <CodeGroup>

  {/* <!-- embedme python/examples/workflows/multi_agents.py --> */}
  ```py Python
  import asyncio
  import sys
  import traceback
  
  from beeai_framework.backend import ChatModel
  from beeai_framework.emitter import EmitterOptions
  from beeai_framework.errors import FrameworkError
  from beeai_framework.tools.search.wikipedia import WikipediaTool
  from beeai_framework.tools.weather import OpenMeteoTool
  from beeai_framework.workflows.agent import AgentWorkflow, AgentWorkflowInput
  from examples.helpers.io import ConsoleReader
  
  
  async def main() -> None:
      llm = ChatModel.from_name("ollama:llama3.1")
      workflow = AgentWorkflow(name="Smart assistant")
  
      workflow.add_agent(
          name="Researcher",
          role="A diligent researcher.",
          instructions="You look up and provide information about a specific topic.",
          tools=[WikipediaTool()],
          llm=llm,
      )
  
      workflow.add_agent(
          name="WeatherForecaster",
          role="A weather reporter.",
          instructions="You provide detailed weather reports.",
          tools=[OpenMeteoTool()],
          llm=llm,
      )
  
      workflow.add_agent(
          name="DataSynthesizer",
          role="A meticulous and creative data synthesizer",
          instructions="You can combine disparate information into a final coherent summary.",
          llm=llm,
      )
  
      reader = ConsoleReader()
  
      reader.write("Assistant 🤖 : ", "What location do you want to learn about?")
      for prompt in reader:
          await (
              workflow.run(
                  inputs=[
                      AgentWorkflowInput(prompt="Provide a short history of the location.", context=prompt),
                      AgentWorkflowInput(
                          prompt="Provide a comprehensive weather summary for the location today.",
                          expected_output="Essential weather details such as chance of rain, temperature and wind. Only report information that is available.",
                      ),
                      AgentWorkflowInput(
                          prompt="Summarize the historical and weather data for the location.",
                          expected_output="A paragraph that describes the history of the location, followed by the current weather conditions.",
                      ),
                  ]
              )
              .on(
                  # Event Matcher -> match agent's 'success' events
                  lambda event: isinstance(event.creator, ChatModel) and event.name == "success",
                  # log data to the console
                  lambda data, event: reader.write(
                      "->Got response from the LLM",
                      "  \n->".join([str(message.content[0].model_dump()) for message in data.value.messages]),
                  ),
                  EmitterOptions(match_nested=True),
              )
              .on(
                  "success",
                  lambda data, event: reader.write(
                      f"->Step '{data.step}' has been completed with the following outcome."
                      f"\n\n{data.state.final_answer}\n\n",
                      data.model_dump(exclude={"data"}),
                  ),
              )
          )
          reader.write("Assistant 🤖 : ", "What location do you want to learn about?")
  
  
  if __name__ == "__main__":
      try:
          asyncio.run(main())
      except FrameworkError as e:
          traceback.print_exc()
          sys.exit(e.explain())
  
  ```

  {/* <!-- embedme typescript/examples/workflows/multiAgents.ts --> */}
  ```ts TypeScript
  import "dotenv/config";
  import { createConsoleReader } from "examples/helpers/io.js";
  import { OpenMeteoTool } from "beeai-framework/tools/weather/openMeteo";
  import { WikipediaTool } from "beeai-framework/tools/search/wikipedia";
  import { AgentWorkflow } from "beeai-framework/workflows/agent";
  import { OllamaChatModel } from "beeai-framework/adapters/ollama/backend/chat";
  
  const workflow = new AgentWorkflow("Smart assistant");
  const llm = new OllamaChatModel("llama3.1");
  
  workflow.addAgent({
    name: "Researcher",
    role: "A diligent researcher",
    instructions: "You look up and provide information about a specific topic.",
    tools: [new WikipediaTool()],
    llm,
  });
  workflow.addAgent({
    name: "WeatherForecaster",
    role: "A weather reporter",
    instructions: "You provide detailed weather reports.",
    tools: [new OpenMeteoTool()],
    llm,
  });
  workflow.addAgent({
    name: "DataSynthesizer",
    role: "A meticulous and creative data synthesizer",
    instructions: "You can combine disparate information into a final coherent summary.",
    llm,
  });
  
  const reader = createConsoleReader();
  reader.write("Assistant 🤖 : ", "What location do you want to learn about?");
  for await (const { prompt } of reader) {
    const { result } = await workflow
      .run([
        { prompt: "Provide a short history of the location.", context: prompt },
        {
          prompt: "Provide a comprehensive weather summary for the location today.",
          expectedOutput:
            "Essential weather details such as chance of rain, temperature and wind. Only report information that is available.",
        },
        {
          prompt: "Summarize the historical and weather data for the location.",
          expectedOutput:
            "A paragraph that describes the history of the location, followed by the current weather conditions.",
        },
      ])
      .observe((emitter) => {
        emitter.on("success", (data) => {
          reader.write(
            `Step '${data.step}' has been completed with the following outcome:\n`,
            data.state?.finalAnswer ?? "-",
          );
        });
      });
  
    reader.write(`Assistant 🤖`, result.finalAnswer);
    reader.write("Assistant 🤖 : ", "What location do you want to learn about?");
  }
  
  ```
  </CodeGroup>

   </Step>
   <Step title="Run the example">

  <CodeGroup>

  ```py Python
  python quickstart.py
  ```

  ```ts TypeScript
  npm exec tsx quickstart.ts
  ```

  </CodeGroup>

   </Step>
</Steps>

Explore more examples in our [Python](https://github.com/i-am-bee/beeai-framework/tree/main/python/examples) and [TypeScript](https://github.com/i-am-bee/beeai-framework/tree/main/typescript/examples) libraries.
