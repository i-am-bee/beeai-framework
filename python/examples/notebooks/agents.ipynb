{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeeAI ReAct agents\n",
    "\n",
    "The BeeAI ReAct agent is a pre-configured implementation of the ReAct (Reasoning and Acting) pattern. It can be customized with tools and instructions to suit different tasks.\n",
    "\n",
    "The ReAct pattern is a framework used in AI models, particularly language models, to separate the reasoning process from the action-taking process. This pattern enhances the model's ability to handle complex queries by enabling it to:\n",
    "\n",
    "- Reason about the problem.\n",
    "- Decide on an action to take.\n",
    "- Observe the result of that action to inform further reasoning and actions.\n",
    "\n",
    "The ReAct agent provides a convenient out-of-the-box agent implementation that makes it easier to build agents using this pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ReAct Agent\n",
    "\n",
    "To configure a ReAct agent, you need to define a ChatModel and construct a BeeAgent.\n",
    "\n",
    "In this example, we won't provide any external tools to the agent. It will rely solely on its own memory to provide answers. This is a basic setup where the agent tries to reason and act based on the context it has built internally.\n",
    "\n",
    "Try modifying the input text in the call to agent.run() to experiment with obtaining different answers. This will help you see how the agent's reasoning and response vary with different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.6)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/mdesmond/projects/beeai-framework/python/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "from beeai_framework.agents.bee.agent import BeeAgent\n",
    "from beeai_framework.agents.types import BeeInput, BeeRunInput, BeeRunOutput\n",
    "from beeai_framework.backend.chat import ChatModel\n",
    "from beeai_framework.emitter.emitter import Emitter, EventMeta\n",
    "from beeai_framework.emitter.types import EmitterOptions\n",
    "from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory\n",
    "\n",
    "# Construct ChatModel\n",
    "chat_model: ChatModel = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "\n",
    "# Construct Agent instance with the chat model\n",
    "agent = BeeAgent(bee_input=BeeInput(llm=chat_model, tools=[], memory=UnconstrainedMemory()))\n",
    "\n",
    "\n",
    "async def process_agent_events(event_data: dict[str, Any], event_meta: EventMeta) -> None:\n",
    "    \"\"\"Process agent events and log appropriately\"\"\"\n",
    "\n",
    "    if event_meta.name == \"error\":\n",
    "        print(\"Agent ðŸ¤– : \", event_data[\"error\"])\n",
    "    elif event_meta.name == \"retry\":\n",
    "        print(\"Agent ðŸ¤– : \", \"retrying the action...\")\n",
    "    elif event_meta.name == \"update\":\n",
    "        print(f\"Agent({event_data['update']['key']}) ðŸ¤– : \", event_data[\"update\"][\"parsedValue\"])\n",
    "\n",
    "\n",
    "# Observe the agent\n",
    "async def observer(emitter: Emitter) -> None:\n",
    "    emitter.on(\"*.*\", process_agent_events, EmitterOptions(match_nested=True))\n",
    "\n",
    "\n",
    "# Run the agent\n",
    "result: BeeRunOutput = await agent.run(\n",
    "    run_input=BeeRunInput(prompt=\"What chemical elements make up a water molecule?\")\n",
    ").observe(observer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tools\n",
    "\n",
    "To go beyond just chatting with an LLM, you can provide tools to the agent. This enables the agent to perform specific tasks and interact with external systems, enhancing its functionality. There are different ways to add tools to the agent:\n",
    "\n",
    "- Built-in tools from the framework: BeeAI provides several built-in tools that you can easily integrate with your agent.\n",
    "- Importing tools from other libraries: You can bring in external tools or APIs to extend your agent's capabilities.\n",
    "- Custom tooling: You can also write your own custom tools tailored to your specific use case.\n",
    "\n",
    "By equipping the agent with these tools, you allow it to perform more complex actions, such as querying databases, interacting with APIs, or manipulating data.\n",
    "\n",
    "## Built-in tools\n",
    "\n",
    "BeeAI comes with several built-in tools that are part of the library, which can be easily imported and added to your agent.\n",
    "\n",
    "In this example, we provide the agent with a weather forecast lookup tool called OpenMeteoTool. With this tool, the agent can retrieve real-time weather data, enabling it to answer weather-related queries with more accuracy.\n",
    "\n",
    "Follow the agent's thoughts and actions to understand how it approaches and solves the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beeai_framework.backend.chat import ChatModel\n",
    "from beeai_framework.tools.weather.openmeteo import OpenMeteoTool\n",
    "\n",
    "chat_model: ChatModel = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "\n",
    "# create an agent using the default LLM and add the OpenMeteoTool that is capable of fetching weather-based information\n",
    "agent = BeeAgent(bee_input=BeeInput(llm=chat_model, tools=[OpenMeteoTool()], memory=UnconstrainedMemory()))\n",
    "\n",
    "\n",
    "# Run the agent\n",
    "result: BeeRunOutput = await agent.run(run_input=BeeRunInput(prompt=\"What's the current weather in London?\")).observe(\n",
    "    observer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported Tools\n",
    "\n",
    "Tools can also be imported from other libraries to extend the functionality of your agent. For example, you can integrate tools from libraries like LangChain to give your agent access to even more capabilities.\n",
    "\n",
    "Hereâ€™s an example showing how to integrate the Wikipedia tool from LangChain, written in long form (without using the @tool decorator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from beeai_framework.agents.bee.agent import BeeAgent\n",
    "from beeai_framework.tools import Tool\n",
    "from beeai_framework.tools.tool import StringToolOutput\n",
    "\n",
    "\n",
    "class LangChainWikipediaToolInput(BaseModel):\n",
    "    query: str = Field(description=\"The topic or question to search for on Wikipedia.\")\n",
    "\n",
    "\n",
    "class LangChainWikipediaTool(Tool):\n",
    "    \"\"\"Adapter class to integrate LangChain's Wikipedia tool with our framework\"\"\"\n",
    "\n",
    "    name = \"Wikipedia\"\n",
    "    description = \"Search factual and historical information from Wikipedia about given topics.\"\n",
    "    input_schema = LangChainWikipediaToolInput\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        wikipedia = WikipediaAPIWrapper()\n",
    "        self.wikipedia = WikipediaQueryRun(api_wrapper=wikipedia)\n",
    "\n",
    "    def _run(self, input: LangChainWikipediaToolInput, _: Any | None = None) -> None:\n",
    "        query = input.query\n",
    "        try:\n",
    "            result = self.wikipedia.run(query)\n",
    "            return StringToolOutput(result=result)\n",
    "        except Exception as e:\n",
    "            print(f\"Wikipedia search error: {e!s}\")\n",
    "            return f\"Error searching Wikipedia: {e!s}\"\n",
    "\n",
    "\n",
    "chat_model: ChatModel = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "agent = BeeAgent(bee_input=BeeInput(llm=chat_model, tools=[LangChainWikipediaTool()], memory=UnconstrainedMemory()))\n",
    "\n",
    "result: BeeRunOutput = await agent.run(\n",
    "    run_input=BeeRunInput(prompt=\"Who is the current president of the European Commission?\")\n",
    ").observe(observer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example can be re-written in a shorter form by adding the @tool decorator, which simplifies the tool definition. Here's how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun  # noqa: F811\n",
    "from langchain_community.utilities import WikipediaAPIWrapper  # noqa: F811\n",
    "\n",
    "from beeai_framework.agents.bee.agent import BeeAgent\n",
    "from beeai_framework.tools import Tool, tool\n",
    "\n",
    "\n",
    "# defining a tool using the `tool` decorator\n",
    "# Note: the pydoc is important as it serves as the tool description to the agent\n",
    "@tool\n",
    "def langchain_wikipedia_tool(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Search factual and historical information, including biography, history, politics, geography, society, culture,\n",
    "    science, technology, people, animal species, mathematics, and other subjects.\n",
    "\n",
    "    Args:\n",
    "        expression: The topic or question to search for on Wikipedia.\n",
    "\n",
    "    Returns:\n",
    "        The information found via searching Wikipedia.\n",
    "    \"\"\"\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    return StringToolOutput(wikipedia.run(expression))\n",
    "\n",
    "\n",
    "# using the tool in an agent\n",
    "chat_model: ChatModel = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "agent = BeeAgent(bee_input=BeeInput(llm=chat_model, tools=[langchain_wikipedia_tool], memory=UnconstrainedMemory()))\n",
    "\n",
    "result: BeeRunOutput = await agent.run(run_input=BeeRunInput(prompt=\"What is the longest living vertebrate?\")).observe(\n",
    "    observer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
