{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjeFsEIdG-2f"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of the BeeAI project is to make AI agents interoperable, regardless of their underlying implementation. The project consists of two key components:\n",
    "- **BeeAI Platform**: The platform to easily discover, run, and compose AI agents from any framework.\n",
    "- **BeeAI Framework**: A production-grade framework for building AI agents in either Python or TypeScript.\n",
    "\n",
    "Detailed information on BeeAI can be found [here](https://beeai.dev/).\n",
    "\n",
    "### What's in this notebook?\n",
    "\n",
    "This notebook demonstrates fundamental usage patterns of the BeeAI Framework in Python. The examples progressively increase in complexity, providing a well-rounded overview of the framework.\n",
    "\n",
    "You can run this notebook on [**Google Colab**](https://colab.research.google.com/). The notebook uses **Ollama** to provide access to a variety of foundation models for remote execution. The notebook will run faster on Colab if you use the free *T4 GPU* option by selecting *Runtime / Change runtime type* in the Colab system menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boch7KbnhMh3"
   },
   "source": [
    "Run the Next Cell to wrap Notebook output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AGDepg4hPUP"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def set_css():\n",
    "    display(HTML(\"\\n<style>\\n pre{\\n white-space: pre-wrap;\\n}\\n</style>\\n\"))\n",
    "\n",
    "\n",
    "get_ipython().events.register(\"pre_run_cell\", set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WWi_9UKOc07"
   },
   "source": [
    "### Install Libraries\n",
    "We start by installing the required dependencies and starting Ollama server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgCirOl1G-2k"
   },
   "outputs": [],
   "source": [
    "%pip install -q langchain_community wikipedia requests==2.32.4 beeai-framework\n",
    "\n",
    "!curl -fsSL https://ollama.com/install.sh | sh > /dev/null\n",
    "!nohup ollama serve >/dev/null 2>&1 &\n",
    "!ollama pull granite3.3:8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNRwKY38OoRS"
   },
   "source": [
    "### Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2CTz2xoG-2k"
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from beeai_framework.backend.chat import ChatModel, ChatModelOutput\n",
    "from beeai_framework.backend.message import AssistantMessage, SystemMessage, UserMessage\n",
    "from beeai_framework.memory.unconstrained_memory import UnconstrainedMemory\n",
    "from beeai_framework.template import PromptTemplate, PromptTemplateInput\n",
    "\n",
    "\n",
    "def object_on_screen(obj):\n",
    "    display(obj)\n",
    "\n",
    "\n",
    "print(\"Imports and credentials completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0p6zai_lG-2l"
   },
   "source": [
    "## Prompt Templates\n",
    "\n",
    "One of the core constructs in the BeeAI Framework is the PromptTemplate. It allows you to dynamically insert data into a prompt before sending it to a language model. BeeAI uses the Mustache templating language for prompt formatting.\n",
    "\n",
    "The following example demonstrates how to create a Retrieval-Augmented Generation (RAG) template and apply it to your data to generate a structured prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tg96kqrtG-2m"
   },
   "outputs": [],
   "source": [
    "print(\"RAG template that includes a Context and a Question:\")\n",
    "\n",
    "\n",
    "# Define the structure of the input data that can passed to the template i.e. the input schema\n",
    "class RAGTemplateInput(BaseModel):\n",
    "    question: str\n",
    "    context: str\n",
    "\n",
    "\n",
    "# Define the prompt template\n",
    "rag_template: PromptTemplate = PromptTemplate(\n",
    "    PromptTemplateInput(\n",
    "        schema=RAGTemplateInput,\n",
    "        template=\"\"\"\n",
    "Context: {{context}}\n",
    "Question: {{question}}\n",
    "\n",
    "Provide a concise answer based on the context. Avoid statements such as 'Based on the context' or 'According to the context' etc. \"\"\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Render the template using an instance of the input model\n",
    "prompt = rag_template.render(\n",
    "    RAGTemplateInput(\n",
    "        question=\"What is the capital of France?\",\n",
    "        context=\"France is a country in Europe. Its capital city is Paris, known for its culture and history.\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print the rendered prompt\n",
    "html = Markdown(prompt)  # convert to HTML\n",
    "object_on_screen(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8hK8yXnG-2n"
   },
   "source": [
    "## More Complex Templates\n",
    "\n",
    "The previous example demonstrated a simple template, but the PromptTemplate class can also handle more complex structures and incorporate conditional logic.\n",
    "\n",
    "The following example showcases a template that includes a question along with a set of detailed search results represented as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EoXPJil0G-2o"
   },
   "outputs": [],
   "source": [
    "print(\"Add to our PromptTemplate a website and some search results:\")\n",
    "\n",
    "\n",
    "# Individual search result\n",
    "class SearchResult(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "# Input specification\n",
    "class SearchTemplateInput(BaseModel):\n",
    "    question: str\n",
    "    results: list[SearchResult]\n",
    "\n",
    "\n",
    "# Define the template, in this instance the template will iterate over the results\n",
    "search_template: PromptTemplate = PromptTemplate(\n",
    "    PromptTemplateInput(\n",
    "        schema=SearchTemplateInput,\n",
    "        template=\"\"\"\n",
    "Search results:\n",
    "{{#results.0}}\n",
    "{{#results}}\n",
    "Title: {{title}}\n",
    "Url: {{url}}\n",
    "Content: {{content}}\n",
    "{{/results}}\n",
    "{{/results.0}}\n",
    "\n",
    "Question: {{question}}\n",
    "Provide a concise answer based on the search results provided.\"\"\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Render the template using an instance of the input model\n",
    "prompt = search_template.render(\n",
    "    SearchTemplateInput(\n",
    "        question=\"What is the capital of France?\",\n",
    "        results=[\n",
    "            SearchResult(\n",
    "                title=\"France\",\n",
    "                url=\"https://en.wikipedia.org/wiki/France\",\n",
    "                content=\"France is a country in Europe. Its capital city is Paris, known for its culture and history.\",\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print the rendered prompt\n",
    "html = Markdown(prompt)  # convert to HTML\n",
    "object_on_screen(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "th7TwXMEG-2o"
   },
   "source": [
    "## The ChatModel\n",
    "\n",
    "Once you have a PromptTemplate and can easily render prompts, youâ€™re ready to start interacting with a model. BeeAI supports a variety of LLMs through the ChatModel interface.\n",
    "\n",
    "In this section, we will use the IBM `Granite 3.1 8B` language model via the Ollama provider.\n",
    "\n",
    "If you haven't set up Ollama yet, follow the [guide on running Granite 3.1 using Ollama](https://www.ibm.com/granite/docs/run/granite-on-mac/granite/) for mac, or for other platforms use the [Ollama documentation](https://ollama.com) and [IBM Granite model page](https://ollama.com/library/granite3.1-dense:8b).\n",
    "\n",
    "Before creating a ChatModel, we need to briefly discuss Messages. The ChatModel operates using message-based interactions, allowing you to structure conversations between the user and the assistant (LLM) naturally."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Construct ChatModel\n",
    "model = ChatModel.from_name(\"ollama:granite3.3\")\n",
    "\n",
    "question1 = \"Briefly explain quantum computing in simple terms with an example.\"\n",
    "message = UserMessage(content=question1)\n",
    "output: ChatModelOutput = await model.run([message])\n",
    "answer1 = output.get_text_content()\n",
    "\n",
    "print(\"Question: \" + question1)\n",
    "print(\"Answer: \" + answer1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "question2 = \"Hello! Can you tell me what is the capital of France?\"\n",
    "message = UserMessage(content=question2)\n",
    "\n",
    "# Create a ChatModel to interface with granite3.1-dense:8b on a local ollama\n",
    "# model = ChatModel.from_name(\"ollama:granite3.1-dense:8b\")\n",
    "\n",
    "output: ChatModelOutput = await model.run([message])\n",
    "answer2 = output.get_text_content()\n",
    "print(\"Question: \" + question2)\n",
    "print()\n",
    "print(\"Answer: \" + answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBJWdkWqG-2p"
   },
   "source": [
    "## Memory\n",
    "The model has provided a response! We can now start to build up a `Memory`. Memory is just a convenient way of storing a set of messages that can be considered as the history of the dialog between the user and the llm.\n",
    "\n",
    "In this next example we will construct a memory from our existing messages and add a new user message. Notice that the new message can implicitly refer to content from prior messages. Internally the `ChatModel` formats all the messages and sends them to the LLM."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "memory = UnconstrainedMemory()\n",
    "question3 = \"If you had to recommend one thing to do there, what would it be?\"\n",
    "await memory.add_many(\n",
    "    [\n",
    "        message,\n",
    "        AssistantMessage(content=output.get_text_content()),\n",
    "        UserMessage(content=question3),\n",
    "    ]\n",
    ")\n",
    "output: ChatModelOutput = await model.run(memory.messages)\n",
    "answer3 = output.get_text_content()\n",
    "\n",
    "print(\"Previous Question: \" + question2)\n",
    "print(\"Previous Answer: \" + answer2)\n",
    "print()\n",
    "print(\"Next Question: \" + question3)\n",
    "print(\"Next Answer: \" + answer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7lnzXosG-2q"
   },
   "source": [
    "## Combining Templates and Messages\n",
    "\n",
    "To use a PromptTemplate with the Granite ChatModel, you can render the template and then place the resulting content into a Message. This allows you to dynamically generate prompts and pass them along as part of the conversation flow."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Some context that the model will use to provide an answer. Source wikipedia: https://en.wikipedia.org/wiki/Ireland\n",
    "context = \"\"\"The geography of Ireland comprises relatively low-lying mountains surrounding a central plain, with several navigable rivers extending inland.\n",
    "Its lush vegetation is a product of its mild but changeable climate which is free of extremes in temperature.\n",
    "Much of Ireland was woodland until the end of the Middle Ages. Today, woodland makes up about 10% of the island,\n",
    "compared with a European average of over 33%, with most of it being non-native conifer plantations.\n",
    "The Irish climate is influenced by the Atlantic Ocean and thus very moderate, and winters are milder than expected for such a northerly area,\n",
    "although summers are cooler than those in continental Europe. Rainfall and cloud cover are abundant.\n",
    "\"\"\"\n",
    "\n",
    "# Lets reuse our RAG template from earlier!\n",
    "question4 = \"How much of Ireland is forested?\"\n",
    "prompt = rag_template.render(RAGTemplateInput(question=question4, context=context))\n",
    "output: ChatModelOutput = await model.run([UserMessage(content=prompt)])\n",
    "answer4 = output.get_text_content()\n",
    "\n",
    "print(\"RAG Template:\")\n",
    "html = Markdown(prompt)  # convert to HTML\n",
    "object_on_screen(html)\n",
    "print(\"Answer: \" + answer4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IT8d3grG-2q"
   },
   "source": [
    "## Structured Outputs\n",
    "\n",
    "Often, you'll want the LLM to produce output in a specific format. This ensures reliable interaction between the LLM and your codeâ€”such as when you need the LLM to generate input for a function or tool. To achieve this, you can use structured output.\n",
    "\n",
    "In the example below, we will prompt Granite to generate a character using a very specific format."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The output structure definition, note the field descriptions that can help the LLM to understand the intention of the field.\n",
    "class CharacterSchema(BaseModel):\n",
    "    name: str = Field(description=\"The name of the character.\")\n",
    "    occupation: str = Field(description=\"The occupation of the character.\")\n",
    "    species: Literal[\"Human\", \"Insectoid\", \"Void-Serpent\", \"Synth\", \"Ethereal\", \"Liquid-Metal\"] = Field(\n",
    "        description=\"The race of the character.\"\n",
    "    )\n",
    "    back_story: str = Field(description=\"Brief backstory of this character.\")\n",
    "\n",
    "\n",
    "question5 = (\n",
    "    \"Create a fantasy sci-fi character for my new game. This character will be the main protagonist, be creative.\"\n",
    ")\n",
    "message = UserMessage(question5)\n",
    "response = await model.run([message], response_format=CharacterSchema)\n",
    "\n",
    "print(\"Question: \" + question5)\n",
    "print(\"Structured output:\")\n",
    "object_on_screen(response.output_structured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFSmFszOG-2r"
   },
   "source": [
    "## System Prompts\n",
    "\n",
    "The SystemMessage is a special message type that can influence the general behavior of an LLM. By including a SystemMessage, you can provide high-level instructions that shape the LLMâ€™s overall response style. The system message typically appears as the first message in the modelâ€™s memory.\n",
    "\n",
    "In the example below, we add a system message that instructs the LLM to speak like a pirate!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pirate_message = \"You are pirate. You always respond using pirate slang.\"\n",
    "system_message = SystemMessage(content=pirate_message)\n",
    "question6 = \"What is a baby hedgehog called?\"\n",
    "message = UserMessage(content=question6)\n",
    "output: ChatModelOutput = await model.run([system_message, message])\n",
    "answer6 = output.get_text_content()\n",
    "\n",
    "print(\"SystemMessage: \" + pirate_message)\n",
    "print()\n",
    "print(\"Question: \" + question6)\n",
    "print(\"Answer: \" + answer6)\n",
    "print()\n",
    "print(\"Demo complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOqHDPyAG-2r"
   },
   "source": [
    "## Learn More\n",
    "\n",
    "Detailed information on BeeAI can be found [here](https://beeai.dev/).\n",
    "\n",
    "In this notebook, you learned the basics of the BeeAI Framework, including PromptTemplates, Messages, ChatModels, Memory, Structured Outputs, and SystemPrompts.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
